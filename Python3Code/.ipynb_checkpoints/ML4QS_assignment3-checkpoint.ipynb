{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import log_loss, confusion_matrix\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHAPTER 2 STUFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chapter2():\n",
    "    path = r'C:/Users/MICK/Desktop/ML4QS/ML4QS/Python3Code/datasets/Assignment3/'\n",
    "    filenames = ['heart_rate', 'motion']\n",
    "    \n",
    "    dfs = []\n",
    "    for i, id_ in enumerate(os.listdir(path)):\n",
    "        print(i, id_)\n",
    "\n",
    "        df = pd.read_csv(f'{path}{id_}/labels.csv')\n",
    "        df.index = pd.to_datetime(df['time'], unit='s')\n",
    "        df = df[df['time'] > 0].drop('time', 1)\n",
    "        df['personid'] = id_\n",
    "        df = df.resample('10s').asfreq().fillna(method='bfill', limit=1).fillna(method='ffill', limit=1)\n",
    "\n",
    "        for filename in filenames:\n",
    "            dfx = pd.read_csv(f'{path}{id_}/{filename}.csv')\n",
    "            dfx.index = pd.to_datetime(dfx['time'], unit='s')\n",
    "            dfx = dfx[dfx['time'] > 0].drop('time', 1)\n",
    "                    \n",
    "            dfmean = dfx.resample('10s').mean()\n",
    "            dfstd = dfx.resample('10s').std()\n",
    "            for col in dfstd.columns:\n",
    "                dfstd.rename(columns={col: col+'_std'}, inplace=True)\n",
    "\n",
    "            dfy = pd.merge(dfmean, dfstd, left_index=True, right_index=True)\n",
    "\n",
    "            df = pd.merge(df, dfy, left_index=True, right_index=True)\n",
    "\n",
    "        dfs.append(df)\n",
    "\n",
    "\n",
    "    dataset = pd.concat(dfs, sort=False)\n",
    "    for label in dataset['label'].unique():\n",
    "        dataset[f'label_{label}'] = np.where(dataset['label'] == label, 1, 0)\n",
    "\n",
    "    dataset = dataset.drop('label', 1)\n",
    "    dataset = dataset[dataset['label_-1.0'] == 0].drop('label_-1.0', 1)\n",
    "    dataset['label_3.0'] = np.where(dataset['label_4.0'] == 1, 1, dataset['label_3.0'])\n",
    "    dataset = dataset.drop('label_4.0', 1)\n",
    "    dataset.reset_index().to_csv('chapter2_result.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1066528\n",
      "1 1360686\n",
      "2 1449548\n",
      "3 1455390\n",
      "4 1818471\n",
      "5 2598705\n",
      "6 2638030\n",
      "7 3509524\n",
      "8 3997827\n",
      "9 4018081\n",
      "10 4314139\n",
      "11 4426783\n",
      "12 46343\n",
      "13 5132496\n",
      "14 5383425\n",
      "15 5498603\n",
      "16 5797046\n",
      "17 6220552\n",
      "18 759667\n",
      "19 7749105\n",
      "20 781756\n",
      "21 8000685\n",
      "22 8173033\n",
      "23 8258170\n",
      "24 844359\n",
      "25 8530312\n",
      "26 8686948\n",
      "27 8692923\n",
      "28 9106476\n",
      "29 9618981\n",
      "30 9961348\n"
     ]
    }
   ],
   "source": [
    "chapter2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'C:/Users/MICK/Desktop/ML4QS/ML4QS/Python3Code/intermediate_datafiles/Assignment3/'\n",
    "df = pd.read_csv(path+'chapter5_result.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.columns[:10]].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['label_0', 'label_1', 'label_2', 'label_3', 'label_5', 'label_-1', 'label_4']\n",
    "df['target'] = df[labels].idxmax(axis=1)\n",
    "df.drop(labels, 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ids = random.sample(list(df['personid'].unique()), 11)\n",
    "testset = df[df['personid'].isin(test_ids)]\n",
    "df = df[~(df['personid'].isin(test_ids))]\n",
    "\n",
    "testset.drop(['time', 'personid'], 1, inplace=True)\n",
    "df.drop(['time', 'personid'], 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold_lightgbm(train_df, test_df, num_folds, stratified = False, debug= False, seed = 1001):\n",
    "    # Divide in training/validation and test data\n",
    "        \n",
    "    print(\"Starting LightGBM. Train shape: {}, test shape: {}\".format(train_df.shape, test_df.shape))\n",
    "    # Cross validation model\n",
    "    if stratified:\n",
    "        folds = StratifiedKFold(n_splits= num_folds, shuffle=True, random_state=seed)\n",
    "    else:\n",
    "        folds = KFold(n_splits= num_folds, shuffle=True, random_state=seed)\n",
    "    # Create arrays and dataframes to store results\n",
    "    oof_preds = np.empty((train_df.shape[0], 7))\n",
    "    sub_preds = np.empty((test_df.shape[0], 7))\n",
    "    feature_importance_split_df = pd.DataFrame()\n",
    "    feature_importance_gain_df = pd.DataFrame()\n",
    "    fold_prediction = test_df.copy()\n",
    "\n",
    "    feats = [f for f in train_df.columns if f not in ['personid', 'time', 'target']]\n",
    "    \n",
    "    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df[feats], train_df['target'])):\n",
    "        train_x, train_y = train_df[feats].iloc[train_idx], train_df['target'].iloc[train_idx]\n",
    "        valid_x, valid_y = train_df[feats].iloc[valid_idx], train_df['target'].iloc[valid_idx]\n",
    "\n",
    "        # LightGBM parameters found by Bayesian optimization\n",
    "        clf = LGBMClassifier(\n",
    "            metric = 'multi_error',\n",
    "            nthread=-1,\n",
    "            n_estimators=10000,\n",
    "            learning_rate=0.15,\n",
    "            num_leaves=4,\n",
    "            colsample_bytree=.1,\n",
    "            subsample=0.4,\n",
    "            max_depth=3,\n",
    "            reg_alpha=1,\n",
    "            reg_lambda=1.2,\n",
    "            min_child_samples = 1000,\n",
    "            min_child_weight = 120,\n",
    "            min_data_per_group = 1000,\n",
    "            min_data_in_bin = 500,\n",
    "            silent=-1,\n",
    "            verbose=-1)\n",
    "\n",
    "        clf.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)], \n",
    "            eval_metric= 'multi_error', verbose= 100, early_stopping_rounds= 200)\n",
    "        \n",
    "        class_order = clf.classes_\n",
    "        \n",
    "        oof_preds[valid_idx] = clf.predict_proba(valid_x, num_iteration=clf.best_iteration_)\n",
    "\n",
    "        test_pred =  clf.predict_proba(test_df[feats], num_iteration=clf.best_iteration_)\n",
    "        sub_preds = test_pred\n",
    "        \n",
    "        #fold_prediction.loc[:, str(seed) + '_fold_' + str(n_fold)] = test_pred\n",
    "\n",
    "        fold_importance_df = pd.DataFrame()\n",
    "        fold_importance_df[\"feature\"] = feats\n",
    "        fold_importance_df[\"importance\"] = clf.booster_.feature_importance(importance_type = 'split')\n",
    "        fold_importance_df[\"fold\"] = n_fold + 1\n",
    "        feature_importance_split_df = pd.concat([feature_importance_split_df, fold_importance_df], axis=0)\n",
    "        \n",
    "        fold_importance_df2 = pd.DataFrame()\n",
    "        fold_importance_df2[\"feature\"] = feats\n",
    "        fold_importance_df2[\"importance\"] = clf.booster_.feature_importance(importance_type = 'gain')\n",
    "        fold_importance_df2[\"fold\"] = n_fold + 1\n",
    "        feature_importance_gain_df = pd.concat([feature_importance_gain_df, fold_importance_df2], axis=0)        \n",
    "        \n",
    "        \n",
    "        #return valid_y, oof_preds[valid_idx]\n",
    "        print('Fold %2d Logloss : %.6f' % (n_fold + 1, log_loss(valid_y, oof_preds[valid_idx].astype(float))))\n",
    "        print()\n",
    "        del clf, train_x, train_y, valid_x, valid_y\n",
    "\n",
    "    print('Full Logloss Training score %.6f' % log_loss(train_df['target'], oof_preds.astype(float)))\n",
    "    print('Full Logloss Test score %.6f' % log_loss(test_df['target'], sub_preds.astype(float)))\n",
    " \n",
    "    display_importances(feature_importance_split_df)\n",
    "    display_importances(feature_importance_gain_df)\n",
    "    return feature_importance_split_df, feature_importance_gain_df, sub_preds, class_order\n",
    "\n",
    "# Display/plot feature importance\n",
    "def display_importances(feature_importance_df_):\n",
    "    cols = feature_importance_df_[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(by=\"importance\", \n",
    "                                                                                                   ascending=False)[:40].index\n",
    "    best_features = feature_importance_df_.loc[feature_importance_df_.feature.isin(cols)]\n",
    "    plt.figure(figsize=(8, 10))\n",
    "    sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False))\n",
    "    plt.title('LightGBM Features (avg over folds)')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "split, gain, preds, order = kfold_lightgbm(df, testset, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adict = {}\n",
    "for i in range(len(order)):\n",
    "    adict[i] = order[i]\n",
    "\n",
    "preds = [adict[i] for i in preds.argmax(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(preds, testset['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
